---
title: "2024 하반기 회고"
date: 2024-11-23 15:31:00 +0900
categories: personal postmortem
---

# 2024 하반기 회고

2024년의 11월입니다. 조금 더 있으면 한 해가 저물기도 하고 회사 업무상으로는 이미 종료 이기도 합니다.
여유가 생긴 지금, 그리고 평가가 종료된 지금, 회고 하기에는 적절한 타이밍입니다.

상반기에는 서비스의 방향성과 정체성을 알아가고 구체화 하던 시기라면 하반기는 **모든것을 재정의** 한 시간이었습니다.
그 일련의 과정을 한번 정리해보려 합니다.

## Meta 의 LLaMa 3.1 공개

7월 23일 Meta 에서 LLaMa 3.1 을 발표합니다.

기존에도 오픈소스로서 자유롭게 사용가능한 모델이 존재 하긴 했으나 3.1 만큼의 대규모 고성능의 모델 (405B) 은 아니었습니다.

추론력이 중요한 우리 서비스에 적합한 모델이 드디어 공개된 것이죠.

그.러.나

고성능의 모델은 그만한 GPU 자원을 필요로 합니다. 
하지만 모두가 알고 있습니다. GPU 는 비싸다는 것. (!!!!Nvidia!!!!)

모델을 사용하려면 일단 GPU 장비가 있어야 하고 그것을 제공 (이용가능하게 하는) 할 수 있는 인프라가 필요합니다.
그리고 보통은 클라우드 자원으로 받아서 구축합니다.

405B 모델을 사용하기 위해서는 보통은 Quantization 을 통해 압축된 상태로 사용합니다. 그러나 성능을 최대한으로 활용하기 위해 16Bit 그대로를 사용한다면

약 405*2 = 910 GB 의 VRAM 을 필요로 합니다.

Nvidia H100 GPU 가 80GB 를 가지고 있어 이론적으로는 12장 정도가 필요합니다. 
그런데 8장단위로 클러스터를 제공하기에 16장을 사용합니다.

H100 Board 1장을 구매하는데 대략 5천만원 정도 입니다. 시스템과 유지보수 합한 가격 예상 되시나요?!

## 상용 LLM 모델 사용에 대한 보안 이슈

이 부분은 사실 많이 아이러니 하다고 생각합니다. 

Cloud 에 올라가 있는 빅데이터를 Cloud 를 통해 서비스로 제공하면서 

Cloud 에 서빙되어 제공하는 LLM Model 을 사용하는 것에 대해서 데이터 유출의 우려를 한다니요.

GPT-3 이후 LLM 모델이 갑자기 대두되면서 세계 모든 기업들이 데이터 우려와 함께 AI 패권과 선점에 대한 경계심이 있다는 것은 이해가 됩니다.

다만.... 시간이 조금 필요한 것이라고 생각합니다.

새로운 기술, 패러다임의 전환, 혼란한 시기에서 책임지고 결단할 용기와 의지가 부족할 수 밖에 없죠.

어쨌든 저희는 이 상황을 반전의 상황으로 극복하려고 했습니다.

우리가 스스로 모델을 finetune 할 수 있고 LLM 에 대해서 더 깊이있게 이해할 수 있는 기회 입니다.

## 위기는 정말 기회인가

8월 말, 정식으로 서비스를 런칭합니다. 어떻게든 꾸역꾸역 우겨넣어서 오픈을 했습니다. 

물론 퀄리티가 스스로에게 만족 할 수준이 아니라는 것은 잘 알고 있었습니다.

그런데 또 한편으로는.... 에이 무슨일 생기겠어? 안이한 생각에 빠져 있었죠.



그래서 일까요? 

C 레벨의 높으신 분들은 그러한 약점을 노리는 것에 도가 텄다... 아니 신기가 있다고 느껴집니다.









LLM 모델을 교체하고 그에 맞게 프롬프팅을 다시 작성하고, 이전 성능 수준으로 끌어 올리기 위한 작업.





제가 소속된 곳에서는 1월 ~ 10월 까지의 성과를 기준으로 평가하고 모든 것을 정리합니다.

11월에는 평가를 하고 사장단 부터 인사를 정리하고 조직을 개편하는 작업을 합니다.

